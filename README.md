Supplementary materials for the paper: <b>CT-SAT: Contextual Transformer for Sequential Audio Tagging

<h3 align="left"><a name="part3">For more details and samples, please see <a href="https://yuanbo2020.github.io/Contextual-Transformer/" 
target="https://yuanbo2020.github.io/Contextual-Transformer/">our homepage</a>.<p></p></h3> 

<br>

# Code and model
If you are interested in the code of CT-SAT or want to train the model, please refer to <a href="https://github.com/Yuanbo2020/GCT#4-ctransformer-contextual-transformer" 
target="https://github.com/Yuanbo2020/GCT#4-ctransformer-contextual-transformer"> the cTransformer (Contextual Transformer) here </a>.<p></p></h3>  

And another model: GCT (Gated Contextual Transformer) on the DCASE2018 dataset can be found <a href="https://github.com/Yuanbo2020/GCT#gct-on-the-dcase2018-dataset" 
target="https://github.com/Yuanbo2020/GCT#gct-on-the-dcase2018-dataset"> here </a>.<p></p></h3> 

# Citation
Please feel free to use the sequential label dataset and consider citing our paper as

```bibtex
@inproceedings{hou22_interspeech,
  author={Yuanbo Hou and Zhaoyi Liu and Bo Kang and Yun Wang and Dick Botteldooren},
  title={{CT-SAT: Contextual Transformer for Sequential Audio Tagging}},
  year=2022,
  booktitle={Proc. Interspeech 2022},
  pages={4147--4151},
  doi={10.21437/Interspeech.2022-196}
}
```







