Supplementary materials for the paper: <b>CT-SAT: Contextual Transformer for Sequential Audio Tagging

<h3 align="left"><a name="part3">For more details and samples, please see <a href="https://yuanbo2020.github.io/Contextual-Transformer/" 
target="https://yuanbo2020.github.io/Contextual-Transformer/">our homepage</a>.<p></p></h3> 

<br>

# Code
If you are interested in the code of CT-SAT or want to train the model, please see <a href="https://github.com/Yuanbo2020/GCT#ctransformer-contextual-transformer" 
target="https://github.com/Yuanbo2020/GCT#ctransformer-contextual-transformer"> here </a>.<p></p></h3>  


# Citation
Please feel free to use the sequential label dataset and consider citing our paper as

```bibtex
@inproceedings{hou22_interspeech,
  author={Yuanbo Hou and Zhaoyi Liu and Bo Kang and Yun Wang and Dick Botteldooren},
  title={{CT-SAT: Contextual Transformer for Sequential Audio Tagging}},
  year=2022,
  booktitle={Proc. Interspeech 2022},
  pages={4147--4151},
  doi={10.21437/Interspeech.2022-196}
}
```







